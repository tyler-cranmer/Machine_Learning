{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "corresponding-transsexual",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7f275e7110ceddd6e18752141d680b59",
     "grade": false,
     "grade_id": "cell-68900ebe59253a81",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# BBC News Classification\n",
    "In this assignment, we will use data from https://www.kaggle.com/c/learn-ai-bbc/overview, which is a Kaggle competition is about categorizing news articles. You will use matrix factorization to predict the category, submit your results to Kaggle for test evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "automated-short",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "from collections import Counter\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.decomposition import NMF\n",
    "import os\n",
    "from itertools import permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "legal-wilderness",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d9925d764d96ff6f075c0bf370c2a221",
     "grade": false,
     "grade_id": "cell-143925419609c138",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Loading train data\n",
    "articles = pd.read_csv(\"data/bbc/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "popular-discussion",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e74ef20f4baf5191350f69739d105d5f",
     "grade": false,
     "grade_id": "cell-75721c43df8bd496",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ArticleId</th>\n",
       "      <th>Text</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1833</td>\n",
       "      <td>worldcom ex-boss launches defence lawyers defe...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>154</td>\n",
       "      <td>german business confidence slides german busin...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1101</td>\n",
       "      <td>bbc poll indicates economic gloom citizens in ...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1976</td>\n",
       "      <td>lifestyle  governs mobile choice  faster  bett...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>917</td>\n",
       "      <td>enron bosses in $168m payout eighteen former e...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1485</th>\n",
       "      <td>857</td>\n",
       "      <td>double eviction from big brother model caprice...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1486</th>\n",
       "      <td>325</td>\n",
       "      <td>dj double act revamp chart show dj duo jk and ...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1487</th>\n",
       "      <td>1590</td>\n",
       "      <td>weak dollar hits reuters revenues at media gro...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1488</th>\n",
       "      <td>1587</td>\n",
       "      <td>apple ipod family expands market apple has exp...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1489</th>\n",
       "      <td>538</td>\n",
       "      <td>santy worm makes unwelcome visit thousands of ...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1490 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      ArticleId                                               Text  \\\n",
       "0          1833  worldcom ex-boss launches defence lawyers defe...   \n",
       "1           154  german business confidence slides german busin...   \n",
       "2          1101  bbc poll indicates economic gloom citizens in ...   \n",
       "3          1976  lifestyle  governs mobile choice  faster  bett...   \n",
       "4           917  enron bosses in $168m payout eighteen former e...   \n",
       "...         ...                                                ...   \n",
       "1485        857  double eviction from big brother model caprice...   \n",
       "1486        325  dj double act revamp chart show dj duo jk and ...   \n",
       "1487       1590  weak dollar hits reuters revenues at media gro...   \n",
       "1488       1587  apple ipod family expands market apple has exp...   \n",
       "1489        538  santy worm makes unwelcome visit thousands of ...   \n",
       "\n",
       "           Category  \n",
       "0          business  \n",
       "1          business  \n",
       "2          business  \n",
       "3              tech  \n",
       "4          business  \n",
       "...             ...  \n",
       "1485  entertainment  \n",
       "1486  entertainment  \n",
       "1487       business  \n",
       "1488           tech  \n",
       "1489           tech  \n",
       "\n",
       "[1490 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Displaying what the data looks like\n",
    "# It has article id, article texts, and category. Here, category is the laabel.\n",
    "articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "challenging-machinery",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6316605b4acb39564a2adc19df14f590",
     "grade": false,
     "grade_id": "cell-7deab6ad2514c41f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'worldcom ex-boss launches defence lawyers defending former worldcom chief bernie ebbers against a battery of fraud charges have called a company whistleblower as their first witness.  cynthia cooper  worldcom s ex-head of internal accounting  alerted directors to irregular accounting practices at the us telecoms giant in 2002. her warnings led to the collapse of the firm following the discovery of an $11bn (£5.7bn) accounting fraud. mr ebbers has pleaded not guilty to charges of fraud and conspiracy.  prosecution lawyers have argued that mr ebbers orchestrated a series of accounting tricks at worldcom  ordering employees to hide expenses and inflate revenues to meet wall street earnings estimates. but ms cooper  who now runs her own consulting business  told a jury in new york on wednesday that external auditors arthur andersen had approved worldcom s accounting in early 2001 and 2002. she said andersen had given a  green light  to the procedures and practices used by worldcom. mr ebber s lawyers have said he was unaware of the fraud  arguing that auditors did not alert him to any problems.  ms cooper also said that during shareholder meetings mr ebbers often passed over technical questions to the company s finance chief  giving only  brief  answers himself. the prosecution s star witness  former worldcom financial chief scott sullivan  has said that mr ebbers ordered accounting adjustments at the firm  telling him to  hit our books . however  ms cooper said mr sullivan had not mentioned  anything uncomfortable  about worldcom s accounting during a 2001 audit committee meeting. mr ebbers could face a jail sentence of 85 years if convicted of all the charges he is facing. worldcom emerged from bankruptcy protection in 2004  and is now known as mci. last week  mci agreed to a buyout by verizon communications in a deal valued at $6.75bn.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's print out a sample article text. You'll also see special characters.\n",
    "articles['Text'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "convertible-exploration",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "acea9c8d64667000fb2d36934d98724d",
     "grade": false,
     "grade_id": "cell-24176612609526b1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['business', 'tech', 'politics', 'sport', 'entertainment'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There are 5 unique categories (labels)\n",
    "articles['Category'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "congressional-broadway",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "04f3c5cf4f746287b5c854101e5dcdd3",
     "grade": false,
     "grade_id": "cell-f83d24c2277e8d4e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([336.,   0., 261.,   0.,   0., 274.,   0., 346.,   0., 273.]),\n",
       " array([0. , 0.4, 0.8, 1.2, 1.6, 2. , 2.4, 2.8, 3.2, 3.6, 4. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's see how many articles are in each category. It shows that the categories are reasonably balanced.\n",
    "plt.hist(articles['Category'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "descending-robinson",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bf9c031038ac98b56c5ed1e48f102231",
     "grade": false,
     "grade_id": "cell-1f0e38ada827728f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "get_feature_names_out## Q1. Text preprocessing\n",
    "As we have done simple EDA, now, let's extract some feaures from the text.\n",
    "Read sklearn document for [TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer) and [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer). Search online about text preprocessing methods based on word count and TF-IDF. \n",
    "### [10 pts] Summarize/explain what those methods are. Why is TF-IDF better than simple word count?\n",
    "(optional: Search and explain other text preprocessing methods such as GloVe or Word2Vec. you can include python snippets on how to use them). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "successful-marine",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from pandas import DataFrame\n",
    "\n",
    "\n",
    "def create_document_term_matrix(message_list, vectorizer):\n",
    "    doc_term_matrix = vectorizer.fit_transform(message_list)\n",
    "    return DataFrame(doc_term_matrix.toarray(), \n",
    "                     columns=vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adolescent-progress",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f94a94c616e2b2a26ab1f25f7f2c110b",
     "grade": true,
     "grade_id": "cell-84f1bc3dc80fc870",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "### Word Count \n",
    "\n",
    "is a NLP algorithm for determining the importance of words used in a document. The algorithm works by taking in a document, in this example two sentences, and tokenizes each unique word in the sentence of the document. Then you create a frequency matrix based on the count of each word comes up in every sentence. You can then use this matrix as training and testing data for classification algorithms. The downside to this type of algorithm is that it does not take into account words that are insignificant to the sentences, like \"is\" or \"to\". So this can make the algorithm give more importance to \"filler\" words in sentences. \n",
    "\n",
    "document = ['My name is Tyler', 'Tyler is taking intro to machine Learning']\n",
    "\n",
    "token_words = ['My', 'name', 'is', 'Tyler', 'taking', 'intro', 'to', 'machine' 'learning']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "protected-length",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>intro</th>\n",
       "      <th>is</th>\n",
       "      <th>learning</th>\n",
       "      <th>machine</th>\n",
       "      <th>my</th>\n",
       "      <th>name</th>\n",
       "      <th>taking</th>\n",
       "      <th>to</th>\n",
       "      <th>tyler</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   intro  is  learning  machine  my  name  taking  to  tyler\n",
       "0      0   1         0        0   1     1       0   0      1\n",
       "1      1   1         1        1   0     0       1   1      1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Word count code example\n",
    "document_1 = ['My name is Tyler', 'Tyler is taking intro to machine Learning']\n",
    "count_vect = CountVectorizer()\n",
    "create_document_term_matrix(document_1, count_vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unnecessary-slovak",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "\n",
    "TF - Term Frequency, is a way to represent how often a word occurs in a document. If there are several occurance of a word in the document, we expect the TF to rise. To find the TF, you take the number of occurances of a word and divide that by the total number of words in the document. \n",
    "\n",
    "$$\n",
    "t f_{i, j}=\\frac{n_{i, j}}{\\sum_{k} n_{i, j}}\n",
    "$$\n",
    "\n",
    "example: \n",
    "\n",
    "      Tyler occurs 2 times in the document and there are 11 total words in the document. so the TF for tyler would be 2/11 = .1818.\n",
    "        \n",
    "IDF - Inverse Data Frequency, is another way to represent how common a word is accross all documents. If a word is used in many documents, then the TF-IDF value would decrease. \n",
    "\n",
    "$$\n",
    "i d f(w)=\\log \\left(\\frac{N}{d f_{t}}\\right)\n",
    "$$\n",
    "      \n",
    "The idf of a word is calculated by taking the log((# of occurances of a word in all documents) / total # of documents))\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "different-october",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>intro</th>\n",
       "      <th>is</th>\n",
       "      <th>learning</th>\n",
       "      <th>machine</th>\n",
       "      <th>my</th>\n",
       "      <th>name</th>\n",
       "      <th>taking</th>\n",
       "      <th>to</th>\n",
       "      <th>tyler</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.409937</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.576152</td>\n",
       "      <td>0.576152</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.409937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.407824</td>\n",
       "      <td>0.290170</td>\n",
       "      <td>0.407824</td>\n",
       "      <td>0.407824</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.407824</td>\n",
       "      <td>0.407824</td>\n",
       "      <td>0.290170</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      intro        is  learning   machine        my      name    taking  \\\n",
       "0  0.000000  0.409937  0.000000  0.000000  0.576152  0.576152  0.000000   \n",
       "1  0.407824  0.290170  0.407824  0.407824  0.000000  0.000000  0.407824   \n",
       "\n",
       "         to     tyler  \n",
       "0  0.000000  0.409937  \n",
       "1  0.407824  0.290170  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example for TF-IDF\n",
    "document_1 = ['My name is Tyler', 'Tyler is taking intro to machine Learning']\n",
    "tfidf_vect = TfidfVectorizer()\n",
    "create_document_term_matrix(document_1, tfidf_vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flying-digest",
   "metadata": {},
   "source": [
    "### Why is TF-IDF better than word count?\n",
    "\n",
    "    - TF-IDF does a better job taking into account the less relavent words like it, the , by. TF-IDF makes rare words more prominent and effectively ignores common words. Unlike word count, TF-IDF creates a normalized count where each word count is divided by the number of documents the word appears in. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vietnamese-lucas",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e762d570499f6206a2226781c4125b11",
     "grade": false,
     "grade_id": "cell-899742352d81079b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In the below example, we will show how to use feature extraction vectorizer. We will show CountVectorizer, but the usage of TfidfVectorizer is also similar. The vectorizer has many options, but `max_features` is most often used. A collection of all words in the all articles is called vocabulary. Since the vocabulary can include so many words, often we want to limit the number of vocabularies in our feature vector. `max_features` is a parameter that sets the limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "charming-throw",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       worldcom ex-boss launches defence lawyers defe...\n",
       "1       german business confidence slides german busin...\n",
       "2       bbc poll indicates economic gloom citizens in ...\n",
       "3       lifestyle  governs mobile choice  faster  bett...\n",
       "4       enron bosses in $168m payout eighteen former e...\n",
       "                              ...                        \n",
       "1485    double eviction from big brother model caprice...\n",
       "1486    dj double act revamp chart show dj duo jk and ...\n",
       "1487    weak dollar hits reuters revenues at media gro...\n",
       "1488    apple ipod family expands market apple has exp...\n",
       "1489    santy worm makes unwelcome visit thousands of ...\n",
       "Name: Text, Length: 1490, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#There are 1490 articles in the train data\n",
    "data_samples = articles['Text']\n",
    "# print(len(data_samples))\n",
    "data_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "broad-thailand",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "behavioral-inflation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take an example of CountVectorizer\n",
    "n_features = 200 # For example, we decided to include only 200 words in our vocabulary (but often it needs bigger number) \n",
    "count_vectorizer = CountVectorizer(max_features=n_features, stop_words=\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "indian-vermont",
   "metadata": {},
   "outputs": [],
   "source": [
    "# .fit_transform() transforms the text data to feature vectors. \n",
    "# Here, the feature matrix from CountVectorizer is a word count vector\n",
    "wordcount = count_vectorizer.fit_transform(data_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dental-disclosure",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1490, 200)\n"
     ]
    }
   ],
   "source": [
    "# This feature matrix has a shape of (# of articles, # max features)\n",
    "# The matrix is a sparse matrix format for computation efficiency.\n",
    "print(wordcount.shape)\n",
    "# print(wordcount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "directed-nicholas",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 0, 0, ..., 0, 0, 1],\n",
       "        [0, 1, 0, ..., 0, 1, 0],\n",
       "        [1, 0, 0, ..., 9, 0, 1],\n",
       "        ...,\n",
       "        [0, 1, 1, ..., 0, 7, 0],\n",
       "        [0, 2, 1, ..., 0, 1, 0],\n",
       "        [1, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Although you can convert a sparse matrix to dense matrix to see the content, \n",
    "# usually we don't want to load a dense matrix of very big matrix (such as a matrix with multiple thousands or millions of rows and cols).\n",
    "# Here, just to show what's inside:\n",
    "wordcount.todense()\n",
    "# It shows a word count fewature matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "encouraging-ethiopia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 1, 1, 0, 2, 3, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "         0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 2, 0, 0, 0,\n",
       "         0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "         0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 1, 5, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#For example, the feature vector for the last third article is this:\n",
    "wordcount.todense()[-3]\n",
    "# It has 0 count the first vocabulary, 1 count for the second vocabulary, 1 count for the third vocabulary, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "abroad-karma",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chief': 29,\n",
       " 'called': 26,\n",
       " 'company': 34,\n",
       " 'firm': 65,\n",
       " 'mr': 122,\n",
       " 'business': 25,\n",
       " 'told': 179,\n",
       " 'new': 128,\n",
       " 'early': 50,\n",
       " 'said': 155,\n",
       " 'given': 73,\n",
       " 'used': 185,\n",
       " 'did': 46,\n",
       " 'star': 170,\n",
       " 'financial': 64,\n",
       " 'hit': 85,\n",
       " 'years': 199,\n",
       " '2004': 5,\n",
       " 'week': 192,\n",
       " 'deal': 43,\n",
       " 'europe': 57,\n",
       " 'economy': 52,\n",
       " 'based': 15,\n",
       " 'research': 152,\n",
       " 'january': 94,\n",
       " 'months': 121,\n",
       " 'economic': 51,\n",
       " 'bank': 14,\n",
       " 'labour': 97,\n",
       " 'minister': 117,\n",
       " 'despite': 45,\n",
       " 'high': 84,\n",
       " 'expected': 59,\n",
       " 'year': 198,\n",
       " '2003': 4,\n",
       " 'growth': 80,\n",
       " 'president': 144,\n",
       " 'going': 74,\n",
       " 'half': 81,\n",
       " 'record': 150,\n",
       " 'making': 110,\n",
       " '10': 1,\n",
       " 'firms': 66,\n",
       " 'including': 88,\n",
       " 'figures': 61,\n",
       " 'rise': 154,\n",
       " 'european': 58,\n",
       " 'bbc': 16,\n",
       " 'nations': 125,\n",
       " 'world': 197,\n",
       " 'service': 164,\n",
       " 'national': 124,\n",
       " 'countries': 36,\n",
       " 'future': 69,\n",
       " '000': 0,\n",
       " 'people': 138,\n",
       " 'country': 37,\n",
       " 'international': 91,\n",
       " 'just': 95,\n",
       " 'director': 48,\n",
       " 'saying': 158,\n",
       " 'place': 140,\n",
       " 'far': 60,\n",
       " 'says': 159,\n",
       " 'china': 30,\n",
       " 'seen': 163,\n",
       " 'government': 77,\n",
       " 'south': 168,\n",
       " 'biggest': 20,\n",
       " '2005': 6,\n",
       " 'took': 180,\n",
       " 'mobile': 118,\n",
       " 'better': 18,\n",
       " 'help': 83,\n",
       " 'phone': 139,\n",
       " 'technology': 176,\n",
       " 'industry': 89,\n",
       " 'michael': 114,\n",
       " 'media': 113,\n",
       " 'news': 129,\n",
       " 'use': 184,\n",
       " 'way': 191,\n",
       " 'good': 75,\n",
       " 'make': 109,\n",
       " 'start': 171,\n",
       " 'users': 186,\n",
       " 'end': 54,\n",
       " 'able': 7,\n",
       " 'number': 130,\n",
       " 'month': 120,\n",
       " 'taking': 173,\n",
       " 'uk': 182,\n",
       " 'film': 62,\n",
       " 'digital': 47,\n",
       " 'life': 101,\n",
       " 'pay': 137,\n",
       " 'went': 193,\n",
       " 'public': 146,\n",
       " 'action': 9,\n",
       " 'added': 10,\n",
       " 'group': 79,\n",
       " 'long': 106,\n",
       " 'big': 19,\n",
       " 'lost': 107,\n",
       " 'howard': 87,\n",
       " 'play': 142,\n",
       " 'time': 178,\n",
       " 'left': 100,\n",
       " 'don': 49,\n",
       " 'think': 177,\n",
       " 'win': 194,\n",
       " 'days': 42,\n",
       " 'game': 70,\n",
       " 'won': 195,\n",
       " 'work': 196,\n",
       " 'like': 102,\n",
       " 'wales': 189,\n",
       " 'second': 160,\n",
       " 'france': 68,\n",
       " 've': 187,\n",
       " 'come': 32,\n",
       " 'ireland': 93,\n",
       " 'campaign': 28,\n",
       " 'right': 153,\n",
       " 'set': 166,\n",
       " 'really': 148,\n",
       " 'hard': 82,\n",
       " 'games': 71,\n",
       " 'got': 76,\n",
       " 'british': 23,\n",
       " 'great': 78,\n",
       " 'britain': 22,\n",
       " 'strong': 172,\n",
       " 'market': 111,\n",
       " 'sales': 156,\n",
       " 'million': 116,\n",
       " 'net': 127,\n",
       " 'office': 132,\n",
       " 'day': 41,\n",
       " 'according': 8,\n",
       " '12': 2,\n",
       " 'blair': 21,\n",
       " 'prime': 145,\n",
       " 'general': 72,\n",
       " 'say': 157,\n",
       " 'security': 162,\n",
       " 'plans': 141,\n",
       " 'law': 99,\n",
       " 'home': 86,\n",
       " 'need': 126,\n",
       " 'want': 190,\n",
       " 'tv': 181,\n",
       " '20': 3,\n",
       " 'recent': 149,\n",
       " 'final': 63,\n",
       " 'tax': 174,\n",
       " 'open': 135,\n",
       " 'later': 98,\n",
       " 'best': 17,\n",
       " 'team': 175,\n",
       " 'came': 27,\n",
       " 'line': 104,\n",
       " 'club': 31,\n",
       " 'old': 133,\n",
       " 'cup': 39,\n",
       " 'players': 143,\n",
       " 'awards': 12,\n",
       " 'music': 123,\n",
       " 'radio': 147,\n",
       " 'know': 96,\n",
       " 'court': 38,\n",
       " 'lot': 108,\n",
       " 'computer': 35,\n",
       " 'match': 112,\n",
       " 'money': 119,\n",
       " 'london': 105,\n",
       " 'united': 183,\n",
       " 'information': 90,\n",
       " 'internet': 92,\n",
       " 'services': 165,\n",
       " 'offer': 131,\n",
       " 'online': 134,\n",
       " 'decision': 44,\n",
       " 'companies': 33,\n",
       " 'software': 167,\n",
       " 'away': 13,\n",
       " 'anti': 11,\n",
       " 'microsoft': 115,\n",
       " 'eu': 56,\n",
       " 'foreign': 67,\n",
       " 'secretary': 161,\n",
       " 'spokesman': 169,\n",
       " 'report': 151,\n",
       " 'brown': 24,\n",
       " 'england': 55,\n",
       " 'video': 188,\n",
       " 'party': 136,\n",
       " 'data': 40,\n",
       " 'election': 53,\n",
       " 'likely': 103}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# .vocabulary_ shows word count for each word in the selected vocabulary in the data\n",
    "count_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "miniature-aerospace",
   "metadata": {},
   "source": [
    "## Q2. Topic Modeling using NMF\n",
    "Below are the starter codes. We will build TopicModeling class which predicts article labels using NMF algorithm.\n",
    "You'll need to complete `factorize` and `predict` methods.\n",
    "\n",
    "### Q2.a Complete factorize function [15 pts]\n",
    "5 pts each for each STEP. You can do the similar from the example above.    \n",
    "**Note:** `self.X` is the train and test data combined. In the STEP2 in `factorize` function, we fit all the data so that the feature matrix contains vocabularies from both trin and test data. You could fit with only texts from train data, but then there might be some vocabularies from test data that don't exist in train data. Since we're not showing the labels to the model, using the vectorizer feature extraction with combined data is ok.\n",
    "\n",
    "### Q2.b Complete STEP4 in predict function [10 pts]\n",
    "self.features is a feature matrix from the tf-idf vectorizer.\n",
    "You can retrieve the fitted feature matrix from train data portion by `self.features[:self.n_tr]`\n",
    "You can calculate `yp_tr` using this train part of the feature matrix. Use the matrix factorization formula (theory) for prediction. It involves some dot products and transpose of matrices. numpy operations require matrices to be numpy array format, which is why we reformatted `tfidf` to `self.features` using `numpy.array` at the end of the `factorize` function.\n",
    "\n",
    "### Q2.c Complete STEP 5 in predict function [15 pts]\n",
    "Map the numeric label values 0,1,2,3,4 from the prediction to category strings. Save the test prediction labels to `self.test['Category']`. For example, `self.test['Category']` may look like:    \n",
    "\n",
    "|     |              |\n",
    "|:----|:------------|     \n",
    "|0    |         sport|    \n",
    "|1    |           tech|     \n",
    "|2    |          sport|    \n",
    "|3    |       business|    \n",
    "|4    |          sport|\n",
    "|...|          |\n",
    "|730  |       business|\n",
    "|731  |  entertainment|\n",
    "|732  |           tech|\n",
    "|733  |       business|\n",
    "|734  |       politics|\n",
    "Name: Category, Length: 735, dtype: object\n",
    "\n",
    "How can you map the integers to the string labels? You can use train data: You can compare the train label string `yt[i]` and the predicted integers from train data `yp_tr[i]` for each sample. Since the model isn't going to predict on train data perfectly, sometimes the match may be wrong. But if you keep track of the matching predicted integer labels for each string label then you can find the majority of the integer index corresponding to the string label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "prompt-seeker",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a3fe3af4030c1d7d4a754a7b74a8273b",
     "grade": true,
     "grade_id": "cell-419ebfff64eeee3c",
     "locked": false,
     "points": 40,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class TopicModeling():\n",
    "    def __init__(self):\n",
    "        self.getdata()\n",
    "        \n",
    "    def getdata(self):\n",
    "        self.train = pd.read_csv(\"data/bbc/train.csv\")\n",
    "        self.test = pd.read_csv(\"data/bbc/test.csv\")\n",
    "        self.X = list(self.train.Text)+list(self.test.Text) # To make the common vocabulary, we will have all texts from train and test data. Don't worry, as long as we don't show test labels, it's not showing the answer.\n",
    "        self.n_tr = len(self.train)\n",
    "        self.n_te = len(self.test)\n",
    "        \n",
    "    def factorize(self,n_features):\n",
    "        self.n_features = n_features\n",
    "        # STEP1. Construct tf-idf vectorizer that accepts max features of n_features. Use parameters max_df=0.95, min_df=2\n",
    "        tfidf_vectorizer = TfidfVectorizer(max_features=n_features, stop_words='english', max_df=0.95, min_df=2)\n",
    "\n",
    "        \n",
    "        # STEP2. Fit the tfidf_vectorizer using the all data X above. Assign the transformed result matrix as tfidf. [5 pts]\n",
    "        tfidf = tfidf_vectorizer.fit_transform(self.X)\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "        # STEP3. Fit the model using sklearn NMF and assign to self.model\n",
    "        self.model = NMF(n_components=5, init='nndsvda', solver = 'mu', beta_loss = 'kullback-leibler', l1_ratio = 0.0)\n",
    "        self.model.fit(tfidf)\n",
    "        self.tfidf = tfidf\n",
    "        self.features = np.array(tfidf.toarray()) #saves the feature matrix in numpy array format. You'll need when predict.\n",
    "        self.best_train_labels = {}\n",
    "        self.best_train_acc = 0\n",
    "    \n",
    "    def predict(self):\n",
    "        # STEP4. Predict labels for train and test data using matrix algebra.\n",
    "        # Refer to the usage in https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html\n",
    "        # The predicted labels are numeric; 0-4\n",
    "        yp_tr = self.features[:self.n_tr]@self.model.components_.T\n",
    "        yp_te = self.features[self.n_tr:,:]@self.model.components_.T\n",
    "        # STEP5. Map the numeric values 0-4 from the prediction to string values of label category.\n",
    "        # You can compare the true labels from train data with yp_tr prediction labels from train data.\n",
    "        # Then you know which number label in yp_tr corresponds which string value. \n",
    "        # update self.test['Category'] to the string labels accordingly.\n",
    "        yt = list(self.train['Category'])\n",
    "        yp = np.argmax(yp_tr, axis=1)\n",
    "        yp_test = np.argmax(yp_te, axis=1)\n",
    "        index = np.array([0,1,2,3,4]);\n",
    "        labels = np.array(['business', 'tech', 'politics', 'sport', 'entertainment']);\n",
    "        perms = list(permutations(index))\n",
    "        best_acc = 0\n",
    "        best_labels = {}\n",
    "        current_labels = {}\n",
    "        for perm in perms:\n",
    "            for i in range(5):\n",
    "                current_labels[perm[i]] = labels[i]\n",
    "                if len(current_labels) == 5:\n",
    "                    current_yp = [current_labels[x] for x in yp]\n",
    "                    curr_acc = accuracy_score(yt,current_yp)\n",
    "                    if curr_acc >= best_acc:\n",
    "                        best_acc = curr_acc\n",
    "                        best_labels = dict(current_labels.items())\n",
    "        self.best_train_labels = best_labels\n",
    "        self.best_train_acc = best_acc\n",
    "        yt_test = [best_labels[x] for x in yp_test]\n",
    "        self.test['Category'] = yt_test\n",
    "        \n",
    "    def save(self): # This function helps to create submission file\n",
    "        if not os.path.isdir('submission'):\n",
    "            os.mkdir('submission')\n",
    "        self.test[[\"ArticleId\",\"Category\"]].to_csv(\"submission/submission_f\"+str(self.n_features)+\".csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "textile-college",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TopicModeling()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "vertical-argument",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TopicModeling()\n",
    "model.factorize(40000)\n",
    "model.predict()\n",
    "# print(model.test)\n",
    "# print(model.best_train_labels)\n",
    "# print(model.best_train_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distinct-newton",
   "metadata": {},
   "source": [
    "## Q3. Tune hyper parameters [10 pts]\n",
    "Change your n_features (for example, between 1000 to 10000). \n",
    "Run prediction which will predict and save\n",
    "Print the train accuracy for each n feature. (You can add print statement for train accuracy in the predict function above)\n",
    "Save the test prediction using save function above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "noble-culture",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a2c8467a59fe71d6c0a4631c739137f1",
     "grade": true,
     "grade_id": "cell-1a56545a0c1a0f6e",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_features = 1000\n",
      "Best permutation and labels: {0: 'business', 1: 'politics', 2: 'sport', 3: 'entertainment', 4: 'tech'}\n",
      "Best Accuracy: 0.9268456375838926 \n",
      "\n",
      "n_features = 2000\n",
      "Best permutation and labels: {0: 'sport', 1: 'politics', 2: 'business', 3: 'entertainment', 4: 'tech'}\n",
      "Best Accuracy: 0.9348993288590604 \n",
      "\n",
      "n_features = 3000\n",
      "Best permutation and labels: {0: 'sport', 1: 'politics', 2: 'business', 3: 'entertainment', 4: 'tech'}\n",
      "Best Accuracy: 0.9402684563758389 \n",
      "\n",
      "n_features = 4000\n",
      "Best permutation and labels: {0: 'sport', 1: 'politics', 2: 'business', 3: 'entertainment', 4: 'tech'}\n",
      "Best Accuracy: 0.9442953020134228 \n",
      "\n",
      "n_features = 5000\n",
      "Best permutation and labels: {0: 'sport', 1: 'politics', 2: 'business', 3: 'entertainment', 4: 'tech'}\n",
      "Best Accuracy: 0.946979865771812 \n",
      "\n",
      "n_features = 6000\n",
      "Best permutation and labels: {0: 'sport', 1: 'politics', 2: 'tech', 3: 'entertainment', 4: 'business'}\n",
      "Best Accuracy: 0.9476510067114094 \n",
      "\n",
      "n_features = 7000\n",
      "Best permutation and labels: {0: 'sport', 1: 'politics', 2: 'tech', 3: 'entertainment', 4: 'business'}\n",
      "Best Accuracy: 0.9483221476510068 \n",
      "\n",
      "n_features = 8000\n",
      "Best permutation and labels: {0: 'sport', 1: 'politics', 2: 'tech', 3: 'entertainment', 4: 'business'}\n",
      "Best Accuracy: 0.9496644295302014 \n",
      "\n",
      "n_features = 9000\n",
      "Best permutation and labels: {0: 'sport', 1: 'politics', 2: 'tech', 3: 'entertainment', 4: 'business'}\n",
      "Best Accuracy: 0.9496644295302014 \n",
      "\n",
      "n_features = 10000\n",
      "Best permutation and labels: {0: 'sport', 1: 'politics', 2: 'tech', 3: 'entertainment', 4: 'business'}\n",
      "Best Accuracy: 0.951006711409396 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "n_features = [1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000]\n",
    "model = TopicModeling()\n",
    "for i, val in enumerate(n_features):\n",
    "    model.factorize(val)\n",
    "    model.predict()\n",
    "    model.save()\n",
    "    print(f'n_features = {val}')\n",
    "    print(f'Best permutation and labels: {model.best_train_labels}')\n",
    "    print(f'Best Accuracy: {model.best_train_acc} \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legitimate-tragedy",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a5e6862192e712c9222274e01828c1cb",
     "grade": false,
     "grade_id": "cell-c05ba034e17ec2d6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Q4. Best results [10 pts]\n",
    "Submit a few test prediction (judge based on train accuracy, although the best train accuracy doesn't mean the best test accuracy) and pick which n_features led to the best test acc. Record the result. Discuss your observations."
   ]
  },
  {
   "cell_type": "raw",
   "id": "functioning-purchase",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "22d746b91f19f4f077b4234e8b6a87f9",
     "grade": true,
     "grade_id": "cell-edcc80f5c5d7a2a6",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "submission_10000 - resulted in a score of .96190\n",
    "submissionf8000 - resulted in a score of .95918\n",
    "submissionf1000 - resulted in a score of .91972\n",
    "\n",
    "The best results came from the models that utilized more top featured vocabulary within all articles for the factorization method. Peramitors within the NMF model also caused a drastic increase in overall accuracy of the models as well. I changed the solver to 'mu' or multiplicative update, init to 'nndsvda' and beta_loss to 'kullback-leibler’. These changes in parameters caused the training accuracy score to go from 75% to around +90% on their own. Combining the changes to NMF and increasing n_features for our factorization method, I achieved a score of 96%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "proved-hometown",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  2,  3,  4,  5],\n",
       "       [ 6,  7,  8,  9, 10]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[1,2,3,4,5],[6,7,8,9,10]])\n",
    "a[0:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "mobile-somalia",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = TopicModeling()\n",
    "b.factorize(1000)\n",
    "n_tr = b.n_tr\n",
    "n_te = b.n_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "engaged-ridge",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2225, 1000)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "unnecessary-zambia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1490, 1000)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.features[:n_tr].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "compliant-performer",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = b.features[n_tr:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "charming-shock",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = b.features[n_tr:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "intermediate-belly",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       ...,\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p == g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "affected-exhaust",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "familiar-computer",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
