{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "09d2eb5fbe70bea445a5bdf28ac0697b",
     "grade": false,
     "grade_id": "cell-e785bf541c3c6566",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Homework 1: PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "50ce8f8109224a024605fa44f6a5958b",
     "grade": false,
     "grade_id": "cell-b5f4e240125e48e2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Problem 1 - Principal Component Analysis\n",
    "---\n",
    "\n",
    "In this problem you'll be implementing Dimensionality reduction using Principal Component Analysis technique. \n",
    "\n",
    "The gist of PCA Algorithm to compute principal components is follows:\n",
    "- Calculate the covariance matrix X of data points.\n",
    "- Calculate eigenvectors and corresponding eigenvalues.\n",
    "- Sort the eigenvectors according to their eigenvalues in decreasing order.\n",
    "- Choose first k eigenvectors which satisfies target explained variance.\n",
    "- Transform the original data of shape m observations times n features into m observations times k selected features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f4571ea879ea92d09a2831fcdac74887",
     "grade": false,
     "grade_id": "cell-eea5ece608ed4ac5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The skeleton for the *PCA* class is below. Scroll down to find more information about your tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "43c4c380877a398b95cc910c3e3f0244",
     "grade": false,
     "grade_id": "cell-0e7782486f935045",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import pickle\n",
    "import gzip\n",
    "import numpy as np\n",
    "import pandas\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "from numpy import linalg as LA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0876c53bb156647022a7851eb5a2003d",
     "grade": false,
     "grade_id": "cell-5874a755dcce55e2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    " from sklearn.preprocessing import StandardScaler\n",
    "    \n",
    "class PCA:\n",
    "    def __init__(self, target_explained_variance=None):\n",
    "        \"\"\"\n",
    "        explained_variance: float, the target level of explained variance\n",
    "        \"\"\"\n",
    "        self.target_explained_variance = target_explained_variance\n",
    "        self.feature_size = -1\n",
    "\n",
    "    def standardize(self, X):\n",
    "        \"\"\"\n",
    "        standardize features using standard scaler\n",
    "        :param X: input data with shape m (# of observations) X n (# of features)\n",
    "        :return: standardized features (Hint: use skleanr's StandardScaler.)\n",
    "        \"\"\"\n",
    "        # Q1. Standardize X using sklearn's StandardScaler. Read the documentation's example. https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler\n",
    "        # Hint: In the example, they used .fit() and .transform() methods separately. You can use another method that has both fit and transform.\n",
    "        # YOUR CODE HERE\n",
    "        std = StandardScaler()\n",
    "        \n",
    "        return std.fit_transform(X)\n",
    "\n",
    "    def compute_mean_vector(self, X_std):\n",
    "        \"\"\"\n",
    "        compute mean vector\n",
    "        :param X_std: transformed data\n",
    "        :return mean vector of shape (# of features,)\n",
    "        \"\"\"\n",
    "        # Q2. return mean vector of shape (# of features,). Hint: averaging over the rows for each column. \n",
    "       \n",
    "        mean_vector = np.mean(X_std, axis=0)  \n",
    "    \n",
    "        return mean_vector\n",
    "\n",
    "    def compute_cov(self, X_std, mean_vec):\n",
    "        \"\"\"\n",
    "        Covariance using mean, (don't use any numpy.cov)\n",
    "        :param X_std:\n",
    "        :param mean_vec:\n",
    "        :return n X n matrix:: covariance matrix\n",
    "        \n",
    "        :return 784 X 784 matrix\n",
    "        \"\"\"\n",
    "        # Q3. Caculate covariance matrix https://en.wikipedia.org/wiki/Covariance_matrix\n",
    "        # Hint: E[(X-mu)T(X-mu)]. You can assume equal probability when calculating the expected value.\n",
    "        cov = np.dot((X_std - mean_vec).T, (X_std-mean_vec)) / (X_std.shape[0] - 1)\n",
    "        \n",
    "        return cov\n",
    "\n",
    "    def compute_eigen_vector(self, cov_mat):\n",
    "        \"\"\"\n",
    "        Eigenvector and eigen values using numpy. Uses numpy's eigenvalue function\n",
    "        :param cov_mat:\n",
    "        :return: (eigen_values, eigen_vector)\n",
    "        \"\"\"\n",
    "        # Q4. Return eigenvalues and engenvectors. \n",
    "        # Hint: Use appropriate function in linalg package. https://numpy.org/doc/stable/reference/routines.linalg.html\n",
    "        # YOUR CODE HERE\n",
    "        e_vals, e_vector = LA.eig(cov_mat)\n",
    "        return (e_vals, e_vector)\n",
    "\n",
    "    def compute_explained_variance(self, eigen_vals):\n",
    "        \"\"\"\n",
    "        Q5. Sort eigen values and compute explained variance ratio.\n",
    "        explained variance informs the amount of information (variance)\n",
    "        can be attributed to each of  the principal components.\n",
    "        :param eigen_vals:\n",
    "        :return: explained variance ratio.\n",
    "        \"\"\"\n",
    "        \n",
    "        idxs = np.argsort(eigen_vals)[::-1]\n",
    "        eigen_vals = eigen_vals[idxs]\n",
    "        \n",
    "        total = eigen_vals.sum()\n",
    "        e_var_ratio = eigen_vals / total\n",
    "        return e_var_ratio\n",
    "\n",
    "    def cumulative_sum(self, var_exp):\n",
    "        \"\"\"\n",
    "        return cumulative sum of explained variance. \n",
    "        :param var_exp: explained variance ratio\n",
    "        :return: cumulative explained variance ratio\n",
    "        \"\"\"\n",
    "        return np.cumsum(var_exp)\n",
    "\n",
    "    def compute_weight_matrix(self, eig_pairs, cum_var_exp):\n",
    "        \"\"\"\n",
    "        compute weight matrix of top principal components conditioned on target\n",
    "        explained variance.\n",
    "        (Hint : use cumilative explained variance ratio and target_explained_variance to find\n",
    "        top components)\n",
    "        \n",
    "        :param eig_pairs: list of tuples containing eigenvalues and eigenvectors, \n",
    "        sorted by eigenvalues in descending order (the biggest eigenvalue and corresponding eigenvectors first).\n",
    "        :param cum_var_exp: cumulative expalined variance by features\n",
    "        :return: weight matrix (the shape of the weight matrix is n X k)\n",
    "        \"\"\"\n",
    "        matrix_w = np.ones((self.feature_size, 1)) \n",
    "        # Q6. In this function, you will implement weight matrix calculation.\n",
    "        # For each iteration, check the cumulative explained variance ratio compared to the target explained variance (see the init vairables)\n",
    "        # then add the eigenvector as column or the matrix_w above.\n",
    "        # matrix_w will have the dimension of (784,1) initially, but each iteration the column will be added until \n",
    "        # the cumulative explained variance reaches the target explained variance.\n",
    "        \n",
    "        idxs = np.argsort(eig_pairs[0])[::-1]\n",
    "        eigenvalues = eig_pairs[0][idxs]\n",
    "        eigenvectors = eig_pairs[1].T\n",
    "        eigenvectors = eigenvectors[idxs]\n",
    "        for i in range(len(cum_var_exp)):\n",
    "            if cum_var_exp[i] >= self.target_explained_variance:\n",
    "                return matrix_w\n",
    "            else:\n",
    "                e_v = np.reshape(eigenvectors[i], (eigenvectors.shape[0],1))\n",
    "                matrix_w = np.append(matrix_w, e_v, axis=1)\n",
    "        return matrix_w\n",
    "\n",
    "    def transform_data(self, X_std, matrix_w):\n",
    "        \"\"\"\n",
    "        transform data to subspace using weight matrix\n",
    "        :param X_std: standardized data\n",
    "        :param matrix_w: weight matrix\n",
    "        :return: data in the subspace\n",
    "        \"\"\"\n",
    "        return X_std.dot(matrix_w)\n",
    "\n",
    "    def fit(self, X):\n",
    "        \"\"\"    \n",
    "        entry point to the transform data to k dimensions\n",
    "        standardize and compute weight matrix to transform data.\n",
    "        The fit functioin returns the transformed features. k is the number of features which cumulative \n",
    "        explained variance ratio meets the target_explained_variance.\n",
    "        :param   m X n dimension: train samples\n",
    "        :return  m X k dimension: subspace data. \n",
    "        \"\"\"\n",
    "    \n",
    "        self.feature_size = X.shape[1]\n",
    "        \n",
    "        \n",
    "        # Multisteps to appomplish the fit function- 16 pts\n",
    "        # step 1. Standardize X to X_std using an appropriate function you implemented above.\n",
    "        X_std = self.standardize(X)\n",
    "        \n",
    "        # step 2. get mean_vec and cov_mat from the appropriate functions from above implementations\n",
    "        mean_vec = self.compute_mean_vector(X_std)\n",
    "        cov_mat = self.compute_cov(X_std, mean_vec)\n",
    "        \n",
    "        # step 3. get eigenvalues and eigenvectors from the implemented function above.\n",
    "        eig = self.compute_eigen_vector(cov_mat)\n",
    "\n",
    "        # step 4. Sort both eig_vals and eig_vecs by descending order in eigenvalues. \n",
    "        # For example, the first 5 elements of the sorted eigenvalues would look like array([170.57702751, 112.84212831,  45.10927112,  40.75001861, 32.99731063])\n",
    "        # and reorder the eigenvector list accordingly.\n",
    "        # Make a list of tuple called eig_pairs\n",
    "        # eig_pairs = [(170.577, the first eigenvector), (112.84, the second eigenvector), ...] (the length of this list is 784)\n",
    "        # Each eigenvector has a dimension of (784,)\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "        \n",
    "        # step 5. get explained variance ratio and cumulated explained variance ratio using functions implemented above.\n",
    "        # Use the variable names below.\n",
    "        var_exp = self.compute_explained_variance(eig[0])\n",
    "        cum_var_exp = self.cumulative_sum(var_exp)\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        # This step calculates the matrix_w\n",
    "        matrix_w = self.compute_weight_matrix(eig_pairs=eig,cum_var_exp=cum_var_exp) \n",
    "        \n",
    "        print(len(matrix_w),len(matrix_w[0]))\n",
    "        return self.transform_data(X_std=X_std, matrix_w=matrix_w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pickle.load(open('./data/fashionmnist/train_images.pkl','rb'))\n",
    "y_train = pickle.load(open('./data/fashionmnist/train_image_labels.pkl','rb'))\n",
    "\n",
    "X_train = X_train[:1500]\n",
    "y_train = y_train[:1500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca = PCA(target_explained_variance=0.99)\n",
    "# std = pca.standardize(X_train)\n",
    "# mv = pca.compute_mean_vector(std)\n",
    "# cov_matrix = pca.compute_cov(std,mv)\n",
    "# eigen = pca.compute_eigen_vector(cov_matrix)\n",
    "# ex_var = pca.compute_explained_variance(eigen[0])\n",
    "# cuml_sum = pca.cumulative_sum(ex_var)\n",
    "# weighted_matrix = pca.compute_weight_matrix(eigen, cuml_sum)\n",
    "\n",
    "# print(std.shape)\n",
    "# print(mv.shape)\n",
    "# print(cov_matrix.shape)\n",
    "# print(eigen[0].shape)\n",
    "# print(eigen[1].shape)\n",
    "# print(cuml_sum.shape)\n",
    "\n",
    "# print(len(eigen))\n",
    "# print(eigan[1].shape)\n",
    "# # print(matrix)\n",
    "# # print(weighted_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a69a61d06f52609eca00d653f99e94b9",
     "grade": false,
     "grade_id": "cell-72c2a70fcc1b5457",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "To Do: Complete helper functions above.\n",
    "\n",
    "complete `fit()` to using all helper functions to find reduced dimension data.\n",
    "\n",
    "Run PCA on *fashion mnist dataset* to reduce the dimension of the data.\n",
    "\n",
    "fashion mnist data consists of samples with *784 dimensions*.\n",
    "\n",
    "Report the reduced dimension $k$ for target explained variance of **0.99**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6b81e50a1e169900d57f6bde5059b554",
     "grade": false,
     "grade_id": "cell-1ca6638507fdcbd6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# X_train = pickle.load(open('./data/fashionmnist/train_images.pkl','rb'))\n",
    "# y_train = pickle.load(open('./data/fashionmnist/train_image_labels.pkl','rb'))\n",
    "\n",
    "# X_train = X_train[:1500]\n",
    "# y_train = y_train[:1500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fa28b9a88044825ca1e35e05796e6a26",
     "grade": false,
     "grade_id": "cell-7ccdd941a2845fa3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "784 410\n"
     ]
    }
   ],
   "source": [
    "pca_handler = PCA(target_explained_variance=0.99)\n",
    "X_train_updated = pca_handler.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
